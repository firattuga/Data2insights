{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "(15,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/m2f3pwyj6s90fdg5k6m6b4k80000gn/T/ipykernel_96783/3460718957.py:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  C_true = pd.read_csv(file_path, delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"xi_plus_covariance.dat\"\n",
    "\n",
    "try:\n",
    "    # Read the file using pandas\n",
    "    # Adjust 'delim_whitespace=True' if the file is space-separated\n",
    "    C_true = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
    "\n",
    "    # Display the first few rows of the data\n",
    "    #print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(C_true.shape)\n",
    "print(C_true[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.995501313152596\n",
      "22.261709941540637\n",
      "\n",
      "--- Part C: Trace(A) = Trace(C_inv * C_hat) ---\n",
      "Expected Trace: 15 (Identity matrix dimension)\n",
      "Measured Trace: 14.9955 +/- 0.0244\n",
      "Conclusion: C_hat is an UNBIASED estimator of C.\n",
      "\n",
      "--- Part E: Trace(B) = Trace(C_hat_inv * C_true) ---\n",
      "Expected Trace (if unbiased): 15\n",
      "Measured Trace: 22.2617 +/- 0.0539\n",
      "\n",
      "Theoretical Bias Factor (Hartlap): 1.0163\n",
      "Theoretical Prediction (15 * Factor): 15.2442\n",
      "\n",
      "Conclusion: C_hat_inv is a BIASED estimator of C_inv.\n"
     ]
    }
   ],
   "source": [
    "psi_true = np.linalg.inv(C_true) #true precision matrix\n",
    "\n",
    "N_measurement = 50\n",
    "N_trial = 1000\n",
    "## A_hat will be Psi_true @ C_hat\n",
    "## B_hat will be Psi_sample @ C_true\n",
    "traces_A = []\n",
    "traces_B = []\n",
    "dim = C_true.shape[0]\n",
    "for trial in range(N_trial):\n",
    "    data = np.random.multivariate_normal(mean = np.zeros(dim),cov = C_true, size = N_measurement)\n",
    "    C_sample = np.cov(data, rowvar=False)\n",
    "\n",
    "    psi_sample_A = np.linalg.inv(C_sample)\n",
    "    A_hat = psi_true @C_sample\n",
    "    traces_A.append(np.trace(A_hat))\n",
    "\n",
    "    psi_sample_B = np.linalg.inv(C_sample)\n",
    "    B_hat = psi_sample_B@C_true\n",
    "    traces_B.append(np.trace(B_hat))\n",
    "    \n",
    "\n",
    "mean_trace_A = np.mean(traces_A)\n",
    "mean_trace_B = np.mean(traces_B)\n",
    "\n",
    "print(mean_trace_A)\n",
    "print(mean_trace_B)\n",
    "\n",
    "#As N_measurement increases, both means should approach dimension but from opposite sides.\n",
    "#Trace(A_hat) is expected to be less than dim while Trace(B_hat) is expected to be greater than dim for finite N_measurement.\n",
    "\n",
    "mean_trace_A = np.mean(traces_A)\n",
    "err_trace_A = np.std(traces_A) / np.sqrt(N_trials)\n",
    "\n",
    "print(\"\\n--- Part C: Trace(A) = Trace(C_inv * C_hat) ---\")\n",
    "print(f\"Expected Trace: {dim} (Identity matrix dimension)\")\n",
    "print(f\"Measured Trace: {mean_trace_A:.4f} +/- {err_trace_A:.4f}\")\n",
    "print(\"Conclusion: C_hat is an UNBIASED estimator of C.\")\n",
    "\n",
    "# Part E Results [cite: 32-35]\n",
    "mean_trace_B = np.mean(traces_B)\n",
    "err_trace_B = np.std(traces_B) / np.sqrt(N_trials)\n",
    "\n",
    "print(\"\\n--- Part E: Trace(B) = Trace(C_hat_inv * C_true) ---\")\n",
    "print(f\"Expected Trace (if unbiased): {dim}\")\n",
    "print(f\"Measured Trace: {mean_trace_B:.4f} +/- {err_trace_B:.4f}\")\n",
    "\n",
    "bias_factor = (N_measurements - 1) / (N_measurements - dim - 2)\n",
    "print(f\"\\nTheoretical Bias Factor (Hartlap): {bias_factor:.4f}\")\n",
    "print(f\"Theoretical Prediction (15 * Factor): {dim * bias_factor:.4f}\")\n",
    "\n",
    "print(\"\\nConclusion: C_hat_inv is a BIASED estimator of C_inv.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage Probability for alpha_true = 0: 0.7763\n",
      "Coverage Probability for alpha_true = 1: 0.7075\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "sigma = 0.5\n",
    "n_experiment = 10000\n",
    "alpha_true_values = [0,1] \n",
    "\n",
    "def calculate_coverage(alpha_true, sigma = 0.5, n_sims = n_experiment):\n",
    "    d_sims = np.random.normal(loc = alpha_true, scale = sigma, size = n_sims)\n",
    "    alpha_grid = np.linspace(-3,3,n_sims)\n",
    "    dx = alpha_grid[1] - alpha_grid[0]\n",
    "\n",
    "    success_count = 0   \n",
    "    for d in d_sims:\n",
    "        log_likelihood = -0.5 * ((d - alpha_grid**3)/sigma)**2\n",
    "        # Normalize (subtract max to prevent underflow)\n",
    "        posterior = np.exp(log_likelihood - np.max(log_likelihood))\n",
    "        norm = trapz(posterior, alpha_grid)\n",
    "        posterior /=norm\n",
    "\n",
    "        sorted_indices = np.argsort(posterior)[::-1]# this will give the indices that would sort the array in descending order\n",
    "        sorted_posterior = posterior[sorted_indices]\n",
    "\n",
    "        #cumulative probability\n",
    "\n",
    "        cumulative_posterior = np.cumsum(sorted_posterior) * dx\n",
    "        idx_cutoff = np.searchsorted(cumulative_posterior, 0.683)\n",
    "        included_indices = sorted_indices[:idx_cutoff + 1]\n",
    "        \n",
    "        min_alpha_cred = np.min(alpha_grid[included_indices])\n",
    "        max_alpha_cred = np.max(alpha_grid[included_indices])\n",
    "\n",
    "        if min_alpha_cred <= alpha_true <= max_alpha_cred:\n",
    "            success_count += 1\n",
    "\n",
    "    return success_count / n_sims\n",
    "\n",
    "\n",
    "coverage_0 = calculate_coverage(alpha_true = 0, sigma = sigma, n_sims = n_experiment)\n",
    "coverage_1 = calculate_coverage(alpha_true = 1, sigma = sigma, n_sims = n_experiment)\n",
    "\n",
    "print(f\"Coverage Probability for alpha_true = 0: {coverage_0:.4f}\")\n",
    "print(f\"Coverage Probability for alpha_true = 1: {coverage_1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/m2f3pwyj6s90fdg5k6m6b4k80000gn/T/ipykernel_96783/1458568530.py:23: DeprecationWarning: 'scipy.integrate.trapz' is deprecated in favour of 'scipy.integrate.trapezoid' and will be removed in SciPy 1.14.0\n",
      "  norm = trapz(posterior, alpha_grid)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage for alpha=0: 0.775\n",
      "Coverage for alpha=1: 0.694\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import trapz\n",
    "def calculate_coverage(alpha_true, sigma=0.5, n_sims=2000):\n",
    "    \"\"\"\n",
    "    Simulates the coverage of Bayesian credible intervals for the model d = alpha^3.\n",
    "    \"\"\"    # \n",
    "    d_sims = np.random.normal(loc=alpha_true**3, scale=sigma, size=n_sims)\n",
    "\n",
    "    # 2. Define a grid for alpha to calculate the posterior\n",
    "    # We need a range that covers the likely values of alpha given the noise.\n",
    "    alpha_grid = np.linspace(-3.0, 3.0, 2000)\n",
    "    dx = alpha_grid[1] - alpha_grid[0]\n",
    "    \n",
    "    hits = 0\n",
    "    \n",
    "    for d in d_sims:\n",
    "        # 3. Calculate Posterior for each experiment\n",
    "        # Likelihood P(d|alpha) = exp( -0.5 * (d - alpha^3)^2 / sigma^2 )\n",
    "        log_likelihood = -0.5 * (d - alpha_grid**3)**2 / sigma**2\n",
    "        \n",
    "        # Normalize (subtract max to prevent underflow)\n",
    "        posterior = np.exp(log_likelihood - np.max(log_likelihood))\n",
    "        norm = trapz(posterior, alpha_grid)\n",
    "        if norm == 0: continue\n",
    "        posterior /= norm\n",
    "        \n",
    "        # 4. Find 68.3% Credible Interval (Highest Posterior Density)\n",
    "        # Sort grid points by probability density (highest to lowest)\n",
    "        sorted_indices = np.argsort(posterior)[::-1]\n",
    "        sorted_post = posterior[sorted_indices]\n",
    "        \n",
    "        # Cumulative sum to find the top 68.3% mass\n",
    "        cumulative_prob = np.cumsum(sorted_post * dx)\n",
    "        \n",
    "        # Find how many points we need to include to reach 68.3%\n",
    "        # (np.searchsorted finds the index where condition is met)\n",
    "        idx_cutoff = np.searchsorted(cumulative_prob, 0.683)\n",
    "        included_indices = sorted_indices[:idx_cutoff+1]\n",
    "        \n",
    "        # 5. Check if true value is inside the credible region\n",
    "        # We define the region by the min and max of the included alpha points\n",
    "        min_alpha_cred = np.min(alpha_grid[included_indices])\n",
    "        max_alpha_cred = np.max(alpha_grid[included_indices])\n",
    "        \n",
    "        if min_alpha_cred <= alpha_true <= max_alpha_cred:\n",
    "            hits += 1\n",
    "            \n",
    "    return hits / n_sims\n",
    "\n",
    "# --- Run the Simulations ---\n",
    "\n",
    "# Case 1: True alpha is 0 (Highly non-linear region)\n",
    "coverage_0 = calculate_coverage(alpha_true=0.0)\n",
    "print(f\"Coverage for alpha=0: {coverage_0:.3f}\")\n",
    "# Expected result: ~0.78 (Over-coverage)\n",
    "\n",
    "# Case 2: True alpha is 1 (Locally linear region)\n",
    "coverage_1 = calculate_coverage(alpha_true=1.0)\n",
    "print(f\"Coverage for alpha=1: {coverage_1:.3f}\")\n",
    "# Expected result: ~0.68 (Matches Frequentist expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
